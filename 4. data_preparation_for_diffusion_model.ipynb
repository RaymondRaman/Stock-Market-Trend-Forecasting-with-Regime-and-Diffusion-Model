{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a41059",
   "metadata": {},
   "source": [
    "### Construct the Full Training and Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04852dd8",
   "metadata": {},
   "source": [
    "### One-Hot Encode the states Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1305ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary library\n",
    "from general_used_functions import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "config_data = load_config_file()\n",
    "selected_unsupervised_model = config_data['selected_unsupervised_model']\n",
    "stock_list = config_data['stock_dict']\n",
    "\n",
    "def load_states_data(stock, selected_model):\n",
    "    DATA_DIR = os.getcwd() + f'/data/{selected_model}_states/{stock}'\n",
    "\n",
    "    # Load the states data\n",
    "    training_states_data = pd.read_excel(DATA_DIR + f'/{stock}_{selected_model}_states(Training).xlsx')\n",
    "    test_states_data = pd.read_excel(DATA_DIR + f'/{stock}_{selected_model}_states(Testing).xlsx')\n",
    "\n",
    "    return training_states_data, test_states_data\n",
    "\n",
    "# Define a function for one-hot encoding the 'states' column\n",
    "def one_hot_encode_states(df):\n",
    "    # Create OneHotEncoder instance; sparse=False will return a dense array\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    # Fit and transform the 'states' column (must be 2D)\n",
    "    state_array = ohe.fit_transform(df[['states']])\n",
    "    # Create column names for each state, e.g., state_0, state_1, etc.\n",
    "    state_feature_names = [f\"state_{i}\" for i in range(state_array.shape[1])]\n",
    "    # Create a DataFrame for the encoded states, retaining the original index\n",
    "    state_df = pd.DataFrame(state_array, columns=state_feature_names, index=df.index)\n",
    "    # Concatenate the new one-hot encoded columns with the original DataFrame and drop the original 'states' column\n",
    "    df_encoded = pd.concat([df.drop(columns=['states']), state_df], axis=1)\n",
    "    return df_encoded\n",
    "\n",
    "# Process each stock and encode the regime states\n",
    "states_encoded_dict = {}\n",
    "for stock in stock_list:\n",
    "    if stock not in selected_unsupervised_model:\n",
    "        continue\n",
    "\n",
    "    # Load training and test states data\n",
    "    training_states_data, test_states_data = load_states_data(stock, selected_unsupervised_model[stock])\n",
    "\n",
    "    # Apply one-hot encoding on both training and test data\n",
    "    training_states_encoded = one_hot_encode_states(training_states_data)\n",
    "    test_states_encoded = one_hot_encode_states(test_states_data)\n",
    "\n",
    "    # Save the encoded data in a dictionary for later use\n",
    "    states_encoded_dict[stock] = (training_states_encoded, test_states_encoded)\n",
    "\n",
    "# Example: print the first few rows of the one-hot encoded training data for AAPL\n",
    "print(states_encoded_dict['AMZN'][0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daffd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature data\n",
    "training_feature_df = load_training_data()\n",
    "testing_feature_df = load_testing_data()\n",
    "\n",
    "def merge_encoded_states_with_features(stock, training_feature_df, testing_feature_df, states_encoded_dict):\n",
    "    # Extract the target columns for the current stock\n",
    "    target_columns = config_data['selected_features_dict'][stock]\n",
    "\n",
    "    # Extract feature data for the current stock\n",
    "    stock_training_features = training_feature_df[stock][target_columns]\n",
    "    stock_testing_features = testing_feature_df[stock][target_columns]\n",
    "\n",
    "    # Retrieve the one-hot encoded regime states\n",
    "    training_states_encoded, testing_states_encoded = states_encoded_dict[stock]\n",
    "\n",
    "    # Merge the feature data with the one-hot encoded regime states\n",
    "    merged_training_data = pd.concat([stock_training_features, training_states_encoded], axis=1)\n",
    "    merged_testing_data = pd.concat([stock_testing_features, testing_states_encoded], axis=1)\n",
    "\n",
    "    # Make sure the merged data has unique column names, lower(), otherwise, drop duplicates\n",
    "    merged_training_data.columns = merged_training_data.columns.str.lower()\n",
    "    merged_testing_data.columns = merged_testing_data.columns.str.lower()\n",
    "    merged_training_data = merged_training_data.loc[:, ~merged_training_data.columns.duplicated()]\n",
    "    merged_testing_data = merged_testing_data.loc[:, ~merged_testing_data.columns.duplicated()]\n",
    "\n",
    "    # Drop the date\n",
    "    merged_training_data = merged_training_data.drop(columns=['date'], errors='ignore')\n",
    "    merged_testing_data = merged_testing_data.drop(columns=['date'], errors='ignore')\n",
    "\n",
    "    return merged_training_data, merged_testing_data\n",
    "\n",
    "def drop_states(df):\n",
    "    # Drop the one-hot encoded state columns\n",
    "    state_columns = [col for col in df.columns if col.startswith('state_')]\n",
    "    df_no_states = df.drop(columns=state_columns, errors='ignore')\n",
    "    return df_no_states\n",
    "\n",
    "for stock in stock_list:\n",
    "    if stock not in selected_unsupervised_model:\n",
    "        continue\n",
    "\n",
    "    # Merge the encoded states with the feature data for the current stock\n",
    "    merged_training_data, merged_testing_data = merge_encoded_states_with_features(stock, training_feature_df, testing_feature_df, states_encoded_dict)\n",
    "\n",
    "    # Saving Directory\n",
    "    training_save_dir = os.getcwd() + f'/ARMD/Data/diffusion_training_data/{stock}'\n",
    "    testing_save_dir = os.getcwd() + f'/ARMD/Data/diffusion_testing_data/{stock}'\n",
    "    os.makedirs(training_save_dir, exist_ok=True)\n",
    "    os.makedirs(testing_save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the data in csv format\n",
    "    merged_training_data.to_csv(training_save_dir + f'/{stock}_training_data.csv', index=False)\n",
    "    merged_testing_data.to_csv(testing_save_dir + f'/{stock}_testing_data.csv', index=False)\n",
    "\n",
    "    # Save the data that does not have the regime states\n",
    "    merged_training_data_no_states = drop_states(merged_training_data)\n",
    "    merged_testing_data_no_states = drop_states(merged_testing_data)\n",
    "\n",
    "    merged_training_data_no_states.to_csv(training_save_dir + f'/{stock}_training_data_no_states.csv', index=False)\n",
    "    merged_testing_data_no_states.to_csv(testing_save_dir + f'/{stock}_testing_data_no_states.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e746e",
   "metadata": {},
   "source": [
    "### Configure File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6067d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated config file for AMZN: ./ARMD/Config/AMZN_armd_config.yaml\n",
      "Generated config file for AMZN without states: ./ARMD/Config/AMZN_armd_config_no_states.yaml\n",
      "Generated config file for GOOGL: ./ARMD/Config/GOOGL_armd_config.yaml\n",
      "Generated config file for GOOGL without states: ./ARMD/Config/GOOGL_armd_config_no_states.yaml\n",
      "Generated config file for MSFT: ./ARMD/Config/MSFT_armd_config.yaml\n",
      "Generated config file for MSFT without states: ./ARMD/Config/MSFT_armd_config_no_states.yaml\n",
      "Generated config file for NVDA: ./ARMD/Config/NVDA_armd_config.yaml\n",
      "Generated config file for NVDA without states: ./ARMD/Config/NVDA_armd_config_no_states.yaml\n",
      "Generated config file for NFLX: ./ARMD/Config/NFLX_armd_config.yaml\n",
      "Generated config file for NFLX without states: ./ARMD/Config/NFLX_armd_config_no_states.yaml\n",
      "Generated config file for TSLA: ./ARMD/Config/TSLA_armd_config.yaml\n",
      "Generated config file for TSLA without states: ./ARMD/Config/TSLA_armd_config_no_states.yaml\n",
      "Generated config file for META: ./ARMD/Config/META_armd_config.yaml\n",
      "Generated config file for META without states: ./ARMD/Config/META_armd_config_no_states.yaml\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "from general_used_functions import load_config_file\n",
    "\n",
    "base_config_path = \"./ARMD/Config/base_config.yaml\"\n",
    "\n",
    "with open(base_config_path, \"r\") as f:\n",
    "    base_config = yaml.safe_load(f)\n",
    "\n",
    "config_data = load_config_file()\n",
    "diffusion_stock_info = config_data['diffusion_stock_info']\n",
    "\n",
    "\n",
    "config_output_dir = \"./ARMD/Config\"\n",
    "\n",
    "# Iterate over each stock and generate a stock-specific config file\n",
    "for stock, params in diffusion_stock_info.items():\n",
    "    # Create a deep copy of the base configuration for each stock\n",
    "    stock_config = copy.deepcopy(base_config)\n",
    "    \n",
    "    # Update model parameters: set feature_size from stock-specific info\n",
    "    stock_config[\"model\"][\"params\"][\"feature_size\"] = params[\"feature_size\"]\n",
    "    \n",
    "    # Update data paths in the dataloader section with the stock-specific merged file paths\n",
    "    stock_config[\"dataloader\"][\"train_dataset\"][\"params\"][\"name\"] = params[\"name\"]\n",
    "    stock_config[\"dataloader\"][\"train_dataset\"][\"params\"][\"data_root\"] = params[\"data_root_train\"]\n",
    "\n",
    "    stock_config[\"dataloader\"][\"test_dataset\"][\"params\"][\"data_root\"] = params[\"data_root_test\"]\n",
    "    stock_config[\"dataloader\"][\"test_dataset\"][\"params\"][\"name\"] = params[\"name\"]\n",
    "\n",
    "    # Update results_folder in solver\n",
    "    stock_config[\"solver\"][\"results_folder\"] = params[\"results_folder\"]\n",
    "    \n",
    "    # Construct the output config filename and save it\n",
    "    config_filename = f\"{stock}_armd_config.yaml\"\n",
    "    config_filepath = os.path.join(config_output_dir, config_filename)\n",
    "    \n",
    "    with open(config_filepath, \"w\") as f:\n",
    "        yaml.dump(stock_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Generated config file for {stock}: {config_filepath}\")\n",
    "\n",
    "    # Generate the config file without the regime states\n",
    "    # Update data paths in the dataloader section with the stock-specific merged file paths, before the .csv\n",
    "    stock_config[\"model\"][\"params\"][\"feature_size\"] = params[\"feature_size_without_states\"]\n",
    "\n",
    "    base, ext = os.path.splitext(params[\"data_root_train\"])\n",
    "    stock_config[\"dataloader\"][\"train_dataset\"][\"params\"][\"name\"] = f'{stock}' + '_no_states' \n",
    "    stock_config[\"dataloader\"][\"train_dataset\"][\"params\"][\"data_root\"] = base + '_no_states' + ext\n",
    "\n",
    "    base, ext = os.path.splitext(params[\"data_root_test\"])\n",
    "    stock_config[\"dataloader\"][\"test_dataset\"][\"params\"][\"name\"] =  f'{stock}' + '_no_states' \n",
    "    stock_config[\"dataloader\"][\"test_dataset\"][\"params\"][\"data_root\"] = base + '_no_states' + ext\n",
    "\n",
    "    # Update results_folder in solver\n",
    "    stock_config[\"solver\"][\"results_folder\"] = params[\"results_folder\"] + '_no_states'\n",
    "\n",
    "    # Construct the output config filename and save it\n",
    "    config_filename_no_states = f\"{stock}_armd_config_no_states.yaml\"\n",
    "    config_filepath_no_states = os.path.join(config_output_dir, config_filename_no_states)\n",
    "    with open(config_filepath_no_states, \"w\") as f:\n",
    "        yaml.dump(stock_config, f, default_flow_style=False)\n",
    "\n",
    "    print(f\"Generated config file for {stock} without states: {config_filepath_no_states}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
